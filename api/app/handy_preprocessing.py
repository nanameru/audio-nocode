"""
HandyéŸ³å£°å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
Handyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®éŸ³å£°å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã‚’Pythonã«ç§»æ¤
"""
import numpy as np
import librosa
import soundfile as sf
import onnxruntime as ort
from collections import deque
from typing import List, Tuple, Dict, Optional
from pathlib import Path


class SileroVAD:
    """Silero VADãƒ¢ãƒ‡ãƒ« (Handyã®silero.rsç§»æ¤)"""

    def __init__(self, model_path: str, threshold: float = 0.3):
        self.session = ort.InferenceSession(model_path)
        self.threshold = threshold
        self.sample_rate = 16000
        self.frame_samples = 480  # 30ms at 16kHz

    def detect(self, frame: np.ndarray) -> float:
        """
        30msãƒ•ãƒ¬ãƒ¼ãƒ ã®éŸ³å£°ç¢ºç‡ã‚’è¿”ã™
        Returns: 0.0-1.0ã®ç¢ºç‡å€¤
        """
        if len(frame) != self.frame_samples:
            raise ValueError(f"Expected {self.frame_samples} samples, got {len(frame)}")

        # ONNXãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œ
        ort_inputs = {"input": frame.reshape(1, -1).astype(np.float32)}
        prob = self.session.run(None, ort_inputs)[0][0][0]
        return float(prob)

    def is_speech(self, frame: np.ndarray) -> bool:
        """éŸ³å£°ã‹ã©ã†ã‹åˆ¤å®š"""
        return self.detect(frame) > self.threshold


class SmoothedVAD:
    """ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°VAD (Handyã®smoothed.rsç§»æ¤)"""

    def __init__(
        self,
        vad: SileroVAD,
        prefill_frames: int = 15,
        hangover_frames: int = 15,
        onset_frames: int = 2
    ):
        self.vad = vad
        self.prefill_frames = prefill_frames
        self.hangover_frames = hangover_frames
        self.onset_frames = onset_frames

        # çŠ¶æ…‹ç®¡ç†
        self.frame_buffer = deque(maxlen=prefill_frames + 1)
        self.hangover_counter = 0
        self.onset_counter = 0
        self.in_speech = False

    def reset(self):
        """çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        self.frame_buffer.clear()
        self.hangover_counter = 0
        self.onset_counter = 0
        self.in_speech = False

    def process_frame(self, frame: np.ndarray) -> Tuple[bool, Optional[np.ndarray]]:
        """
        ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†
        Returns: (is_speech, audio_data)
            - is_speech: ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡ºåŠ›ã«å«ã‚ã‚‹ã‹
            - audio_data: å‡ºåŠ›ã™ã‚‹éŸ³å£°ãƒ‡ãƒ¼ã‚¿ï¼ˆNoneã®å ´åˆã¯å‡ºåŠ›ã—ãªã„ï¼‰
        """
        # ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
        self.frame_buffer.append(frame.copy())

        # VADåˆ¤å®š
        is_voice = self.vad.is_speech(frame)

        # çŠ¶æ…‹é·ç§»
        if not self.in_speech and is_voice:
            # éŸ³å£°é–‹å§‹ã®å¯èƒ½æ€§
            self.onset_counter += 1
            if self.onset_counter >= self.onset_frames:
                # éŸ³å£°é–‹å§‹ç¢ºå®š - prefillãƒãƒƒãƒ•ã‚¡ã‚’å«ã‚ã¦è¿”ã™
                self.in_speech = True
                self.hangover_counter = self.hangover_frames
                self.onset_counter = 0
                # ãƒãƒƒãƒ•ã‚¡å…¨ä½“ã‚’é€£çµã—ã¦è¿”ã™
                return True, np.concatenate(list(self.frame_buffer))
            return False, None

        elif self.in_speech and is_voice:
            # éŸ³å£°ç¶™ç¶š
            self.hangover_counter = self.hangover_frames
            return True, frame

        elif self.in_speech and not is_voice:
            # éŸ³å£°çµ‚äº†ã®å¯èƒ½æ€§
            if self.hangover_counter > 0:
                self.hangover_counter -= 1
                return True, frame  # hangoveræœŸé–“ã¯å‡ºåŠ›ã«å«ã‚ã‚‹
            else:
                # éŸ³å£°çµ‚äº†ç¢ºå®š
                self.in_speech = False
                return False, None

        else:  # not in_speech and not is_voice
            # ç„¡éŸ³ç¶™ç¶š
            self.onset_counter = 0
            return False, None


class HandyPreprocessor:
    """HandyéŸ³å£°å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰"""

    def __init__(self, vad_model_path: str = "/app/models/silero_vad.onnx"):
        self.vad_model_path = vad_model_path
        self.target_sr = 16000
        self.frame_duration_ms = 30
        self.frame_samples = int(self.target_sr * self.frame_duration_ms / 1000)  # 480

        # VADãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–æ™‚ã«ãƒ­ãƒ¼ãƒ‰
        self.silero_vad = SileroVAD(vad_model_path)
        print(f"âœ… Handy preprocessor initialized with VAD model: {vad_model_path}")

    def process(
        self,
        input_path: str,
        output_path: str,
        vad_enabled: bool = True,
        vad_threshold: float = 0.3,
        onset_frames: int = 2,
        prefill_frames: int = 15,
        hangover_frames: int = 15,
        enable_visualization: bool = True
    ) -> Dict:
        """
        éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰å‡¦ç†

        Args:
            input_path: å…¥åŠ›éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            output_path: å‡ºåŠ›éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            vad_enabled: VADã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
            vad_threshold: Silero VADé–¾å€¤ (0.0-1.0)
            onset_frames: éŸ³å£°é–‹å§‹åˆ¤å®šãƒ•ãƒ¬ãƒ¼ãƒ æ•°
            prefill_frames: éŸ³å£°é–‹å§‹å‰ãƒãƒƒãƒ•ã‚¡ãƒ•ãƒ¬ãƒ¼ãƒ æ•°
            hangover_frames: éŸ³å£°çµ‚äº†å¾Œä¿æŒãƒ•ãƒ¬ãƒ¼ãƒ æ•°
            enable_visualization: ã‚¹ãƒšã‚¯ãƒˆãƒ«å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ

        Returns:
            metadata: å‡¦ç†ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        """
        print(f"ğŸ™ï¸  Handy preprocessing: {input_path}")
        print(f"   VAD: {vad_enabled}, threshold: {vad_threshold}")
        print(f"   Onset: {onset_frames}, Prefill: {prefill_frames}, Hangover: {hangover_frames}")

        # 1. éŸ³å£°èª­ã¿è¾¼ã¿ï¼ˆãƒ¢ãƒãƒ©ãƒ«åŒ–ï¼‰
        audio, original_sr = librosa.load(input_path, sr=None, mono=True)
        print(f"ğŸ“¥ Loaded: {len(audio)} samples at {original_sr}Hz")

        # 2. ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (16kHz)
        if original_sr != self.target_sr:
            print(f"ğŸ”„ Resampling {original_sr}Hz â†’ {self.target_sr}Hz...")
            audio = librosa.resample(audio, orig_sr=original_sr, target_sr=self.target_sr)

        # 3. VADé©ç”¨
        processed_audio = []
        vad_segments = []

        if vad_enabled:
            print(f"ğŸ¯ Applying VAD...")

            # SmoothedVADã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é©ç”¨ï¼‰
            self.silero_vad.threshold = vad_threshold
            smoothed_vad = SmoothedVAD(
                self.silero_vad,
                prefill_frames=prefill_frames,
                hangover_frames=hangover_frames,
                onset_frames=onset_frames
            )

            # ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½ã§å‡¦ç†
            segment_start = None
            for i in range(0, len(audio), self.frame_samples):
                frame = audio[i:i + self.frame_samples]

                # æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                if len(frame) < self.frame_samples:
                    frame = np.pad(frame, (0, self.frame_samples - len(frame)))

                is_speech, speech_data = smoothed_vad.process_frame(frame)

                if is_speech and speech_data is not None:
                    if segment_start is None:
                        segment_start = len(processed_audio) / self.target_sr if processed_audio else 0
                    processed_audio.append(speech_data)
                elif segment_start is not None:
                    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆçµ‚äº†
                    vad_segments.append({
                        'start': segment_start,
                        'end': (len(np.concatenate(processed_audio)) if processed_audio else 0) / self.target_sr
                    })
                    segment_start = None

            # æœ€å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’é–‰ã˜ã‚‹
            if segment_start is not None and processed_audio:
                vad_segments.append({
                    'start': segment_start,
                    'end': len(np.concatenate(processed_audio)) / self.target_sr
                })

            if processed_audio:
                processed_audio = np.concatenate(processed_audio)
                print(f"âœ… VAD: {len(vad_segments)} segments, {len(processed_audio)} samples")
            else:
                print(f"âš ï¸  VAD: No speech detected, using original audio")
                processed_audio = audio
        else:
            processed_audio = audio

        # 4. ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆ1ç§’æœªæº€ãªã‚‰1.25ç§’ã«ï¼‰
        if len(processed_audio) < self.target_sr:
            target_length = int(self.target_sr * 1.25)
            print(f"ğŸ“Œ Padding: {len(processed_audio)} â†’ {target_length} samples")
            processed_audio = np.pad(processed_audio, (0, target_length - len(processed_audio)))

        # 5. ã‚¹ãƒšã‚¯ãƒˆãƒ«å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        visualization = None
        if enable_visualization:
            visualization = self._generate_spectrum(processed_audio)
            print(f"ğŸ“Š Visualization: {len(visualization)} buckets")

        # 6. ä¿å­˜
        sf.write(output_path, processed_audio, self.target_sr)
        print(f"ğŸ’¾ Saved: {output_path}")

        return {
            'original_sr': int(original_sr),
            'resampled_sr': self.target_sr,
            'original_duration': float(len(audio) / original_sr),
            'processed_duration': float(len(processed_audio) / self.target_sr),
            'vad_enabled': vad_enabled,
            'vad_segments': vad_segments if vad_enabled else [],
            'num_segments': len(vad_segments) if vad_enabled else 0,
            'visualization': visualization
        }

    def _generate_spectrum(self, audio: np.ndarray, num_buckets: int = 16) -> List[float]:
        """
        FFTã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ†æï¼ˆHandyã®visualizer.rsç§»æ¤ï¼‰
        80-4000Hzã€å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã€dBæ­£è¦åŒ–
        """
        # STFTã§ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ è¨ˆç®—
        D = librosa.stft(audio, n_fft=512, hop_length=256, window='hann')
        S = np.abs(D)

        # dBå¤‰æ›
        S_db = librosa.amplitude_to_db(S, ref=np.max)

        # å‘¨æ³¢æ•°ãƒ“ãƒ³ã‚’å–å¾—
        freqs = librosa.fft_frequencies(sr=self.target_sr, n_fft=512)

        # 16ãƒã‚±ãƒƒãƒˆã«é›†ç´„ï¼ˆ80-4000Hzã€å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        buckets = []
        for i in range(num_buckets):
            # å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ« (Handyã¨åŒã˜)
            log_start = (i / num_buckets) ** 2
            log_end = ((i + 1) / num_buckets) ** 2

            freq_start = 80 + (4000 - 80) * log_start
            freq_end = 80 + (4000 - 80) * log_end

            # è©²å½“ã™ã‚‹å‘¨æ³¢æ•°ãƒ“ãƒ³ã®å¹³å‡
            mask = (freqs >= freq_start) & (freqs < freq_end)
            if np.any(mask):
                bucket_db = np.mean(S_db[mask, :])
                # -55dB ~ -8dB ã‚’ 0~1 ã«æ­£è¦åŒ–
                normalized = (bucket_db - (-55)) / ((-8) - (-55))
                normalized = np.clip(normalized, 0, 1)
                # ã‚²ã‚¤ãƒ³è£œæ­£ã¨ã‚«ãƒ¼ãƒ–é©ç”¨ (Handyã¨åŒã˜)
                final_value = (normalized * 1.3) ** 0.7
                buckets.append(float(np.clip(final_value, 0, 1)))
            else:
                buckets.append(0.0)

        return buckets


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³
_handy_preprocessor: Optional[HandyPreprocessor] = None

def get_handy_preprocessor() -> HandyPreprocessor:
    """Handyå‰å‡¦ç†ã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _handy_preprocessor
    if _handy_preprocessor is None:
        _handy_preprocessor = HandyPreprocessor()
    return _handy_preprocessor
